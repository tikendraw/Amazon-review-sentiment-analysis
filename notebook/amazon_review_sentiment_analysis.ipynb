{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "view-in-github",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tikendraw/Amazon-review-sentiment-analysis/blob/main/amazon-review-sentiment-analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c16cfc6",
      "metadata": {
        "id": "1c16cfc6"
      },
      "source": [
        "# Amazon Reviews for Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fac0c72",
      "metadata": {
        "id": "3fac0c72"
      },
      "source": [
        "## Objective\n",
        "\n",
        "Here we will be Building ML and DL models to predict the Polarity of reviews.\n",
        "We will be performing series of experiments with different models to achieve the best classification metrics.(while not abusing the machine we have)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ec49b73",
      "metadata": {
        "id": "8ec49b73"
      },
      "source": [
        "## About Dataset\n",
        "[Dataset here](https://www.kaggle.com/datasets/kritanjalijain/amazon-reviews)\n",
        "\n",
        "\n",
        "### OVERVIEW\n",
        "Contains 34,686,770 Amazon reviews from 6,643,669 users on 2,441,053 products, from the Stanford Network Analysis Project (SNAP). This subset contains 1,800,000 training samples and 200,000 testing samples in each polarity sentiment.\n",
        "\n",
        "### ORIGIN\n",
        "The Amazon reviews dataset consists of reviews from amazon. The data span a period of 18 years, including ~35 million reviews up to March 2013. Reviews include product and user information, ratings, and a plaintext review. For more information, please refer to the following paper: J. McAuley and J. Leskovec. Hidden factors and hidden topics: understanding rating dimensions with review text. RecSys, 2013.\n",
        "\n",
        "### DESCRIPTION\n",
        "The Amazon reviews polarity dataset is constructed by taking review score 1 and 2 as negative, and 4 and 5 as positive. Samples of score 3 is ignored. In the dataset, class 1 is the negative and class 2 is the positive. Each class has 1,800,000 training samples and 200,000 testing samples.\n",
        "\n",
        "If you need help extracting the train.csv and test.csv files check out the starter code.\n",
        "\n",
        "The files train.csv and test.csv contain all the training samples as comma-separated values.\n",
        "\n",
        "The CSVs contain polarity, title, text. These 3 columns in them, correspond to class index (1 or 2), review title and review text.\n",
        "\n",
        "polarity - 1 for negative and 2 for positive\n",
        "title - review heading\n",
        "text - review body\n",
        "The review title and text are escaped using double quotes (\"), and any internal double quote is escaped by 2 double quotes (\"\"). New lines are escaped by a backslash followed with an \"n\" character, that is \"\\n\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29a25034",
      "metadata": {
        "id": "29a25034"
      },
      "outputs": [],
      "source": [
        "# ! git clone https://github.com/tikendraw/funcyou.git -q\n",
        "# ! pip install funcyou/\n",
        "# ! rm -rf funcyou\n",
        "# ! pip install tensorflow_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8e4GevPRvy90",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e4GevPRvy90",
        "outputId": "3eace966-e7d3-445a-8ddd-e766c0320719"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install contractions -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3f06a868",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f06a868",
        "outputId": "3e319b05-1d99-4670-cddf-fa50ad6dd8f1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-01 15:39:23.112360: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-09-01 15:39:24.445659: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tf version: 2.12.1\n",
            "GPU: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-01 15:39:27.220833: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-09-01 15:39:27.561068: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n"
          ]
        }
      ],
      "source": [
        "import datetime\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "import sys\n",
        "import tarfile\n",
        "import time\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "\n",
        "import contractions\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, TextVectorization\n",
        "from tensorflow_hub import KerasLayer\n",
        "\n",
        "try:\n",
        "    import funcyou\n",
        "except ImportError:\n",
        "    # Install funcyou\n",
        "    !pip install git+https://github.com/tikendraw/funcyou.git@main -q\n",
        "\n",
        "# Importing useful functions from funcyou\n",
        "from funcyou.dataset import download_kaggle_resource\n",
        "from funcyou.plot import distplot_axis, plot_history, compare_histories\n",
        "from funcyou.preprocessing.text import IntegerVectorizer\n",
        "from funcyou.sklearn.metrics import calculate_results\n",
        "\n",
        "print('Tf version:', tf.__version__)\n",
        "print('GPU:', len(tf.config.list_physical_devices('GPU')))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "750e2f16",
      "metadata": {
        "id": "750e2f16"
      },
      "outputs": [],
      "source": [
        "def set_global_seed(seed):\n",
        "    tf.random.set_seed(seed)\n",
        "    global random_state\n",
        "    random_state = check_random_state(seed)\n",
        "\n",
        "\n",
        "# Set the seed value\n",
        "SEED = 42 # should be same as seed in config file\n",
        "\n",
        "# Call the function to set the seeds\n",
        "set_global_seed(SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "JoiHXi71fwLG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoiHXi71fwLG",
        "outputId": "d0ccd049-9c18-4b42-9405-444a9b76db15"
      },
      "outputs": [],
      "source": [
        "# download and enter the repo to access other files\n",
        "\n",
        "colab = 'google.colab' in str(get_ipython())\n",
        "\n",
        "if colab:\n",
        "    os.system('git clone https://github.com/tikendraw/Amazon-review-sentiment-analysis.git')\n",
        "\n",
        "    if os.getcwd() != '/content/Amazon-review-sentiment-analysis/notebook':\n",
        "        os.chdir('Amazon-review-sentiment-analysis/notebook')\n",
        "    print('Current working directory: ',os.getcwd())\n",
        "\n",
        "\n",
        "    # load google drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "35cec4d3",
      "metadata": {
        "id": "35cec4d3"
      },
      "outputs": [],
      "source": [
        "cur_dir = Path(os.getcwd())\n",
        "model_dir = Path(os.getcwd()).parent /'model'\n",
        "data_dir = Path(os.getcwd()).parent /'dataset'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "db919e15",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db919e15",
        "outputId": "74abeef6-ed13-4109-a801-f086838b712c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/t/aproject/Amazon-review-sentiment-analysis/notebook\n",
            "/home/t/aproject/Amazon-review-sentiment-analysis/model\n",
            "/home/t/aproject/Amazon-review-sentiment-analysis/dataset\n"
          ]
        }
      ],
      "source": [
        "print(cur_dir)\n",
        "print(model_dir)\n",
        "print(data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NX4nMoR_KGax",
      "metadata": {
        "id": "NX4nMoR_KGax"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "Ao1oDevPpp3z",
      "metadata": {
        "id": "Ao1oDevPpp3z"
      },
      "outputs": [],
      "source": [
        "# Download the data if you don't have locally\n",
        "dataset_url = 'https://www.kaggle.com/datasets/kritanjalijain/amazon-reviews'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "XSkPxCeYqyQS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSkPxCeYqyQS",
        "outputId": "8ebfc0e3-c6b7-4b8d-eec5-cb796af6bb61"
      },
      "outputs": [],
      "source": [
        "if colab:\n",
        "    download_kaggle_resource(dataset_url, data_dir, kaggle_json_path = '/content/kaggle.json')\n",
        "\n",
        "    from zipfile import ZipFile\n",
        "    with ZipFile(data_dir/'amazon-reviews'/'amazon-reviews.zip') as f:\n",
        "        f.extractall(data_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5b0fa02",
      "metadata": {
        "id": "a5b0fa02"
      },
      "source": [
        "# Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d2fc7d74",
      "metadata": {
        "id": "d2fc7d74"
      },
      "outputs": [],
      "source": [
        "#reading data\n",
        "df = pl.read_csv(data_dir/'train.csv',new_columns = ['polarity', 'title','text'])  # gives TextFileReader, which is iterable with chunks of 1000 rows.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "132384b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "132384b1",
        "outputId": "6e76ea7e-2cf5-45ef-b7c5-7e1c7de38b9a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr > th,\n",
              ".dataframe > tbody > tr > td {\n",
              "  text-align: right;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (9, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>describe</th><th>polarity</th><th>title</th><th>text</th></tr><tr><td>str</td><td>f64</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>3.599999e6</td><td>&quot;3599999&quot;</td><td>&quot;3599999&quot;</td></tr><tr><td>&quot;null_count&quot;</td><td>0.0</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td></tr><tr><td>&quot;mean&quot;</td><td>1.5</td><td>null</td><td>null</td></tr><tr><td>&quot;std&quot;</td><td>0.5</td><td>null</td><td>null</td></tr><tr><td>&quot;min&quot;</td><td>1.0</td><td>&quot;&quot;</td><td>&quot;\u0003this is the b…</td></tr><tr><td>&quot;max&quot;</td><td>2.0</td><td>&quot;♦ LOVE IT ♦&quot;</td><td>&quot;…were Marvin a…</td></tr><tr><td>&quot;median&quot;</td><td>1.0</td><td>null</td><td>null</td></tr><tr><td>&quot;25%&quot;</td><td>1.0</td><td>null</td><td>null</td></tr><tr><td>&quot;75%&quot;</td><td>2.0</td><td>null</td><td>null</td></tr></tbody></table></div>"
            ],
            "text/plain": [
              "shape: (9, 4)\n",
              "┌────────────┬────────────┬─────────────┬───────────────────────────────────┐\n",
              "│ describe   ┆ polarity   ┆ title       ┆ text                              │\n",
              "│ ---        ┆ ---        ┆ ---         ┆ ---                               │\n",
              "│ str        ┆ f64        ┆ str         ┆ str                               │\n",
              "╞════════════╪════════════╪═════════════╪═══════════════════════════════════╡\n",
              "│ count      ┆ 3.599999e6 ┆ 3599999     ┆ 3599999                           │\n",
              "│ null_count ┆ 0.0        ┆ 0           ┆ 0                                 │\n",
              "│ mean       ┆ 1.5        ┆ null        ┆ null                              │\n",
              "│ std        ┆ 0.5        ┆ null        ┆ null                              │\n",
              "│ min        ┆ 1.0        ┆             ┆ \u0003this is the best toy in the wro…  │\n",
              "│ max        ┆ 2.0        ┆ ♦ LOVE IT ♦ ┆ …were Marvin and Tami not the be… │\n",
              "│ median     ┆ 1.0        ┆ null        ┆ null                              │\n",
              "│ 25%        ┆ 1.0        ┆ null        ┆ null                              │\n",
              "│ 75%        ┆ 2.0        ┆ null        ┆ null                              │\n",
              "└────────────┴────────────┴─────────────┴───────────────────────────────────┘"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "Jsn520qsxj6C",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "Jsn520qsxj6C",
        "outputId": "9a8f59e5-2e44-4efe-c5bf-155e66d4984a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr > th,\n",
              ".dataframe > tbody > tr > td {\n",
              "  text-align: right;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (1, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>polarity</th><th>title</th><th>text</th></tr><tr><td>u32</td><td>u32</td><td>u32</td></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td></tr></tbody></table></div>"
            ],
            "text/plain": [
              "shape: (1, 3)\n",
              "┌──────────┬───────┬──────┐\n",
              "│ polarity ┆ title ┆ text │\n",
              "│ ---      ┆ ---   ┆ ---  │\n",
              "│ u32      ┆ u32   ┆ u32  │\n",
              "╞══════════╪═══════╪══════╡\n",
              "│ 0        ┆ 0     ┆ 0    │\n",
              "└──────────┴───────┴──────┘"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check for nulls and drop if any\n",
        "df.null_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a0ml-HHdxvxo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0ml-HHdxvxo",
        "outputId": "8170b6ca-1ac3-4ca8-835c-31a554125068"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#drop nulls\n",
        "df.drop_nulls()\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c672dec5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "c672dec5",
        "outputId": "9822a960-2382-4c7b-f38b-cbbd70d89884"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr > th,\n",
              ".dataframe > tbody > tr > td {\n",
              "  text-align: right;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (2, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>polarity</th><th>counts</th></tr><tr><td>i64</td><td>u32</td></tr></thead><tbody><tr><td>1</td><td>1800000</td></tr><tr><td>2</td><td>1799999</td></tr></tbody></table></div>"
            ],
            "text/plain": [
              "shape: (2, 2)\n",
              "┌──────────┬─────────┐\n",
              "│ polarity ┆ counts  │\n",
              "│ ---      ┆ ---     │\n",
              "│ i64      ┆ u32     │\n",
              "╞══════════╪═════════╡\n",
              "│ 1        ┆ 1800000 │\n",
              "│ 2        ┆ 1799999 │\n",
              "└──────────┴─────────┘"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# checking for classs imbalance\n",
        "df['polarity'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c168d8cf",
      "metadata": {
        "id": "c168d8cf"
      },
      "source": [
        "# We will map the polarity between 0 for negative sentiment to 1 for positive sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f5cac18d",
      "metadata": {
        "id": "f5cac18d"
      },
      "outputs": [],
      "source": [
        "df = df.with_columns([\n",
        "                    pl.col('polarity').apply(lambda x: 0 if x == 1 else 1).alias('polarity')\n",
        "                     ])\n",
        "\n",
        "df = df.with_columns([\n",
        "                     pl.col('polarity').cast(pl.Int16, strict=False).alias('polarity')\n",
        "                     ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "be973a53",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "be973a53",
        "outputId": "ef59836f-b251-47cf-892e-63035a8124ae"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr > th,\n",
              ".dataframe > tbody > tr > td {\n",
              "  text-align: right;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (10, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>polarity</th><th>title</th><th>text</th></tr><tr><td>i16</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>0</td><td>&quot;Super Ape Inna…</td><td>&quot;I picked a cop…</td></tr><tr><td>0</td><td>&quot;MISSING A SCEN…</td><td>&quot;Do not buy thi…</td></tr><tr><td>0</td><td>&quot;Requires paid …</td><td>&quot;This sounded l…</td></tr><tr><td>1</td><td>&quot;One of the ver…</td><td>&quot;I have read ot…</td></tr><tr><td>1</td><td>&quot;One of the bes…</td><td>&quot;This product i…</td></tr><tr><td>1</td><td>&quot;This was Great…</td><td>&quot;I relistened t…</td></tr><tr><td>1</td><td>&quot;Sex &amp; the Sing…</td><td>&quot;I love movies …</td></tr><tr><td>0</td><td>&quot;Dull Read&quot;</td><td>&quot;This book was …</td></tr><tr><td>0</td><td>&quot;ATC 2K&quot;</td><td>&quot;The sound on t…</td></tr><tr><td>0</td><td>&quot;do not buy it.…</td><td>&quot;i had great ex…</td></tr></tbody></table></div>"
            ],
            "text/plain": [
              "shape: (10, 3)\n",
              "┌──────────┬───────────────────────────────────┬───────────────────────────────────┐\n",
              "│ polarity ┆ title                             ┆ text                              │\n",
              "│ ---      ┆ ---                               ┆ ---                               │\n",
              "│ i16      ┆ str                               ┆ str                               │\n",
              "╞══════════╪═══════════════════════════════════╪═══════════════════════════════════╡\n",
              "│ 0        ┆ Super Ape Inna Jungle by Lee Per… ┆ I picked a copy up in some San F… │\n",
              "│ 0        ┆ MISSING A SCENE!!                 ┆ Do not buy this DVD! I am very f… │\n",
              "│ 0        ┆ Requires paid membership after a… ┆ This sounded like a great idea, … │\n",
              "│ 1        ┆ One of the very best!             ┆ I have read other accounts of th… │\n",
              "│ …        ┆ …                                 ┆ …                                 │\n",
              "│ 1        ┆ Sex & the Single Girl             ┆ I love movies from this era and … │\n",
              "│ 0        ┆ Dull Read                         ┆ This book was such a disappointm… │\n",
              "│ 0        ┆ ATC 2K                            ┆ The sound on this helmet cam is … │\n",
              "│ 0        ┆ do not buy it...!                 ┆ i had great expectations on this… │\n",
              "└──────────┴───────────────────────────────────┴───────────────────────────────────┘"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.sample(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de08710a",
      "metadata": {
        "id": "de08710a"
      },
      "source": [
        "## Note: We will be combining text and title columns . makes more sense."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "26892e90",
      "metadata": {
        "id": "26892e90"
      },
      "outputs": [],
      "source": [
        "df = df.with_columns([\n",
        "    (pl.col('title')+' ' + pl.col('text')).alias('review')\n",
        "])\n",
        "\n",
        "df = df.with_columns([\n",
        "    pl.col('review').str.to_lowercase()\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "9f9586d2",
      "metadata": {
        "id": "9f9586d2"
      },
      "outputs": [],
      "source": [
        "# df = df.with_columns([\n",
        "#     pl.col('title').apply(lambda x: len(str(x).split())).alias('title_len'),\n",
        "# ])\n",
        "\n",
        "# df = df.with_columns([\n",
        "#     pl.col('text').apply(lambda x: len(str(x).split())).alias('text_len'),\n",
        "# ])\n",
        "\n",
        "# df = df.with_columns([\n",
        "#     pl.col('review').apply(lambda x: len(str(x).split())).alias('review_len')\n",
        "# ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "5d86a82a",
      "metadata": {
        "id": "5d86a82a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# plt.figure(figsize=(15,5))\n",
        "# distplot_axis(df['title_len'].to_numpy())\n",
        "\n",
        "# plt.figure(figsize=(15,5))\n",
        "# distplot_axis(df['text_len'].to_numpy())\n",
        "\n",
        "# plt.figure(figsize=(15,5))\n",
        "# distplot_axis(df['review_len'].to_numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "419c7412",
      "metadata": {
        "id": "419c7412"
      },
      "outputs": [],
      "source": [
        "df = df.select(['review', 'polarity'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "e0aca943",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "e0aca943",
        "outputId": "c0885f1b-549c-445d-a1ba-6f4771c745d6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr > th,\n",
              ".dataframe > tbody > tr > td {\n",
              "  text-align: right;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>review</th><th>polarity</th></tr><tr><td>str</td><td>i16</td></tr></thead><tbody><tr><td>&quot;the best sound…</td><td>1</td></tr><tr><td>&quot;amazing! this …</td><td>1</td></tr><tr><td>&quot;excellent soun…</td><td>1</td></tr><tr><td>&quot;remember, pull…</td><td>1</td></tr><tr><td>&quot;an absolute ma…</td><td>1</td></tr></tbody></table></div>"
            ],
            "text/plain": [
              "shape: (5, 2)\n",
              "┌───────────────────────────────────┬──────────┐\n",
              "│ review                            ┆ polarity │\n",
              "│ ---                               ┆ ---      │\n",
              "│ str                               ┆ i16      │\n",
              "╞═══════════════════════════════════╪══════════╡\n",
              "│ the best soundtrack ever to anyt… ┆ 1        │\n",
              "│ amazing! this soundtrack is my f… ┆ 1        │\n",
              "│ excellent soundtrack i truly lik… ┆ 1        │\n",
              "│ remember, pull your jaw off the … ┆ 1        │\n",
              "│ an absolute masterpiece i am qui… ┆ 1        │\n",
              "└───────────────────────────────────┴──────────┘"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "23ab7ccb",
      "metadata": {
        "id": "23ab7ccb"
      },
      "outputs": [],
      "source": [
        "df.write_csv(data_dir / 'preprocessed_df.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "ef9509d5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "ef9509d5",
        "outputId": "882bd594-9e93-454d-f070-4a4675e8d9f5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr > th,\n",
              ".dataframe > tbody > tr > td {\n",
              "  text-align: right;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>review</th><th>polarity</th></tr><tr><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;the best sound…</td><td>1</td></tr><tr><td>&quot;amazing! this …</td><td>1</td></tr><tr><td>&quot;excellent soun…</td><td>1</td></tr><tr><td>&quot;remember, pull…</td><td>1</td></tr><tr><td>&quot;an absolute ma…</td><td>1</td></tr></tbody></table></div>"
            ],
            "text/plain": [
              "shape: (5, 2)\n",
              "┌───────────────────────────────────┬──────────┐\n",
              "│ review                            ┆ polarity │\n",
              "│ ---                               ┆ ---      │\n",
              "│ str                               ┆ i64      │\n",
              "╞═══════════════════════════════════╪══════════╡\n",
              "│ the best soundtrack ever to anyt… ┆ 1        │\n",
              "│ amazing! this soundtrack is my f… ┆ 1        │\n",
              "│ excellent soundtrack i truly lik… ┆ 1        │\n",
              "│ remember, pull your jaw off the … ┆ 1        │\n",
              "│ an absolute masterpiece i am qui… ┆ 1        │\n",
              "└───────────────────────────────────┴──────────┘"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pl.read_csv(data_dir/'preprocessed_df.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4c2a876",
      "metadata": {
        "id": "d4c2a876"
      },
      "source": [
        "## Preprocessing text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "4375da5e",
      "metadata": {
        "id": "4375da5e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Compile the regular expressions outside the function for better performance\n",
        "PUNCTUATION_REGEX = re.compile(r'[^\\w\\s]')\n",
        "DIGIT_REGEX = re.compile(r'\\d')\n",
        "SPECIAL_CHARACTERS_REGEX = re.compile(r'[#,@,&]')\n",
        "MULTIPLE_SPACES_REGEX = re.compile(r'\\s+')\n",
        "\n",
        "def clean_text(x: str) -> str:\n",
        "    expanded_text = contractions.fix(x)  # Expand contractions\n",
        "    text = PUNCTUATION_REGEX.sub(' ', expanded_text.lower())  # Remove punctuation after lowering\n",
        "    text = DIGIT_REGEX.sub('', text)  # Remove digits\n",
        "    # Remove special characters (#,@,&)\n",
        "    text = SPECIAL_CHARACTERS_REGEX.sub('', text)\n",
        "    # Remove multiple spaces with single space\n",
        "    text = MULTIPLE_SPACES_REGEX.sub(' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "# Compile regex patterns for better performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "e1142967",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "e1142967",
        "outputId": "d88fb88c-4fb3-4c0c-fa89-c415a3be1be1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'i am going to market you want to come huh'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "q = \"i'm going to market.you wanna come?huh?\"\n",
        "clean_text(q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "Un2OoPUTS3Mt",
      "metadata": {
        "id": "Un2OoPUTS3Mt"
      },
      "outputs": [],
      "source": [
        "counter_object_filepath = model_dir / 'counter.pkl'\n",
        "\n",
        "save_dir = Path('/content/drive/MyDrive/models/amazon_sentiment')\n",
        "if colab:\n",
        "    counter_object_filepath = save_dir / 'counter.pkl'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "bc8c952a",
      "metadata": {
        "id": "bc8c952a"
      },
      "outputs": [],
      "source": [
        "# a = IntegerVectorizer(preprocessing_func=clean_text, min_freq=3) # 19 min  , contraction fix takes most of the time\n",
        "# a.adapt(df['review'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "b7uFmdEOTnKt",
      "metadata": {
        "id": "b7uFmdEOTnKt"
      },
      "outputs": [],
      "source": [
        "# # save the counter object\n",
        "# counter_object = a.vocab.counter  #Vocab size: 261933\n",
        "\n",
        "# with open(counter_object_filepath, 'wb') as output_file:\n",
        "#     pickle.dump(counter_object, output_file)\n",
        "#     print('file saved at',counter_object_filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "46830b95",
      "metadata": {
        "id": "46830b95"
      },
      "outputs": [],
      "source": [
        "with open(counter_object_filepath, 'rb') as output_file:\n",
        "    counter_object = pickle.load(output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "3bc639ba",
      "metadata": {
        "id": "3bc639ba"
      },
      "outputs": [],
      "source": [
        "# Get the least common words\n",
        "least_common_words = counter_object.most_common()[::-1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "sleu-ogsY4p8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sleu-ogsY4p8",
        "outputId": "49232859-2119-4510-8373-5a454a543e8e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('eclair', 14),\n",
              " ('krown', 14),\n",
              " ('pti', 14),\n",
              " ('winterbourne', 14),\n",
              " ('stracher', 14),\n",
              " ('zekk', 14),\n",
              " ('slection', 14),\n",
              " ('fiv', 14),\n",
              " ('kreh', 14),\n",
              " ('kaneda', 14),\n",
              " ('xhr', 14),\n",
              " ('funning', 14),\n",
              " ('guenivere', 14),\n",
              " ('aiki', 14),\n",
              " ('erring', 14),\n",
              " ('sundiver', 14),\n",
              " ('retirements', 14),\n",
              " ('carryin', 14),\n",
              " ('wfp', 14),\n",
              " ('lunt', 14),\n",
              " ('hydrangeas', 14),\n",
              " ('coombe', 14),\n",
              " ('precisa', 14),\n",
              " ('reeler', 14),\n",
              " ('sesson', 14),\n",
              " ('handsomest', 14),\n",
              " ('milimeter', 14),\n",
              " ('gastar', 14),\n",
              " ('suggestible', 14),\n",
              " ('splitted', 14),\n",
              " ('koboi', 14),\n",
              " ('kemprecos', 14),\n",
              " ('shoppin', 14),\n",
              " ('axillary', 14),\n",
              " ('brawlers', 14),\n",
              " ('whodunnits', 14),\n",
              " ('anquish', 14),\n",
              " ('convened', 14),\n",
              " ('alsoa', 14),\n",
              " ('unopen', 14),\n",
              " ('openner', 14),\n",
              " ('jomini', 14),\n",
              " ('referb', 14),\n",
              " ('gimmee', 14),\n",
              " ('inteligencia', 14),\n",
              " ('fcs', 14),\n",
              " ('gumy', 14),\n",
              " ('corrals', 14),\n",
              " ('baleful', 14),\n",
              " ('procedings', 14),\n",
              " ('chipettes', 14),\n",
              " ('highliter', 14),\n",
              " ('perfers', 14),\n",
              " ('incrdible', 14),\n",
              " ('tullock', 14),\n",
              " ('stofen', 14),\n",
              " ('boons', 14),\n",
              " ('whiskas', 14),\n",
              " ('cushings', 14),\n",
              " ('casefiles', 14),\n",
              " ('heigel', 14),\n",
              " ('sqrt', 14),\n",
              " ('getthis', 14),\n",
              " ('emir', 14),\n",
              " ('margalo', 14),\n",
              " ('cloner', 14),\n",
              " ('crocidile', 14),\n",
              " ('pulk', 14),\n",
              " ('rembered', 14),\n",
              " ('porducts', 14),\n",
              " ('oversteer', 14),\n",
              " ('multihull', 14),\n",
              " ('motet', 14),\n",
              " ('reimbursment', 14),\n",
              " ('cyric', 14),\n",
              " ('relapsed', 14),\n",
              " ('esalen', 14),\n",
              " ('daimajin', 14),\n",
              " ('centerpoint', 14),\n",
              " ('slithered', 14),\n",
              " ('deadbolts', 14),\n",
              " ('guidline', 14),\n",
              " ('pierdas', 14),\n",
              " ('spurling', 14),\n",
              " ('mustapha', 14),\n",
              " ('vsc', 14),\n",
              " ('madalyn', 14),\n",
              " ('schönberg', 14),\n",
              " ('poetica', 14),\n",
              " ('studyguide', 14),\n",
              " ('hald', 14),\n",
              " ('birn', 14),\n",
              " ('spilsbury', 14),\n",
              " ('kuper', 14),\n",
              " ('ratcliffe', 14),\n",
              " ('readthatagain', 14),\n",
              " ('simle', 14),\n",
              " ('trem', 14),\n",
              " ('erro', 14),\n",
              " ('razones', 14)]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f = 100_000\n",
        "counter_object.most_common()[f:f+100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "719ae47c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "719ae47c",
        "outputId": "bf608df2-98be-4f7a-b4e6-c14003d22913"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                counter_object: 29.3 MiB\n",
            "            least_common_words:  6.8 MiB\n",
            "            ThreadPoolExecutor:  1.6 KiB\n",
            "                       Counter:  1.6 KiB\n",
            "               TfidfVectorizer:  1.6 KiB\n",
            "                 MultinomialNB:  1.6 KiB\n",
            "                      Pipeline:  1.6 KiB\n",
            "                         Dense:  1.6 KiB\n",
            "                       Dropout:  1.6 KiB\n",
            "                     Embedding:  1.6 KiB\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "def sizeof_fmt(num, suffix='B'):\n",
        "    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
        "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
        "        if abs(num) < 1024.0:\n",
        "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
        "        num /= 1024.0\n",
        "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
        "\n",
        "for name, size in sorted(((name, sys.getsizeof(value)) for name, value in list(\n",
        "                          locals().items())), key= lambda x: -x[1], reverse = False)[:10]:\n",
        "    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ec6ee32",
      "metadata": {
        "id": "9ec6ee32"
      },
      "source": [
        "# Text vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "e6f6c05a",
      "metadata": {
        "id": "e6f6c05a"
      },
      "outputs": [],
      "source": [
        "MAX_TOKEN = 100_000\n",
        "OUTPUT_SEQUENCE_LENGTH = 175  # limiting reviews to 175 words"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b150ae0c",
      "metadata": {
        "id": "b150ae0c"
      },
      "source": [
        "creating a dictionary of words with counts helps me create text vectorizer alot faster .\n",
        "\n",
        "\n",
        "it only took 3min 24sec to create the dictionary from 3.6 million entries which is alot faster if you try to\n",
        "adapt  TextVectorization to the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "1771469a",
      "metadata": {
        "id": "1771469a"
      },
      "outputs": [],
      "source": [
        "text_vectorizer = TextVectorization(max_tokens=MAX_TOKEN, standardize='lower_and_strip_punctuation',\n",
        "                                   split='whitespace',\n",
        "                                    ngrams= None ,\n",
        "                                    output_mode='int',\n",
        "                                    output_sequence_length=OUTPUT_SEQUENCE_LENGTH,\n",
        "                                    pad_to_max_tokens=True,\n",
        "                                    vocabulary = list(counter_object.keys())[:MAX_TOKEN-2]) # -2 FOR 'PAD' AND 'UNK' TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "1f9708f5",
      "metadata": {
        "id": "1f9708f5"
      },
      "outputs": [],
      "source": [
        "random_review = \"WHo the duck are you? aren't you a goose?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "489427a3",
      "metadata": {
        "id": "489427a3"
      },
      "outputs": [],
      "source": [
        "word_to_id = tf.keras.layers.StringLookup(vocabulary=text_vectorizer.get_vocabulary(), mask_token='', oov_token='[UNK]')\n",
        "id_to_word = tf.keras.layers.StringLookup(vocabulary=text_vectorizer.get_vocabulary(), mask_token='', oov_token='[UNK]', invert=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "4bRul9U1Ua0c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4bRul9U1Ua0c",
        "outputId": "64fc9493-d21f-4379-d384-09e75f787bbe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'who the duck are you are not you a goose'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clean_text(random_review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "336021c9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "336021c9",
        "outputId": "a43a3bc2-5447-44bf-f9a7-805096bd5117"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "random Review:  WHo the duck are you? aren't you a goose?\n",
            "random Review length:  41\n",
            "-------\n",
            "\n",
            "vectorized review:  tf.Tensor(\n",
            "[[   91     2  8076    58    57     1    57    11 30011     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0]], shape=(1, 175), dtype=int64)\n",
            "Reverse vectorized review:  tf.Tensor(\n",
            "[[b'who' b'the' b'duck' b'are' b'you' b'[UNK]' b'you' b'a' b'goose' b''\n",
            "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
            "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
            "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
            "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
            "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
            "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
            "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
            "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
            "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
            "  b'' b'' b'']], shape=(1, 175), dtype=string)\n",
            "-------\n",
            "\n",
            "Vocabulary_length:  100000\n",
            "Most frequent words:  ['', '[UNK]', 'the', 'best', 'soundtrack', 'ever', 'to', 'anything', 'i', 'am', 'reading', 'a', 'lot', 'of', 'reviews', 'saying', 'that', 'this', 'is', 'game', 'and', 'figured', 'would', 'write', 'review', 'disagree', 'bit', 'in', 'my', 'opinino', 'yasunori', 'mitsuda', 's', 'ultimate', 'masterpiece', 'music', 'timeless', 'been', 'listening', 'it', 'for', 'years', 'now', 'its', 'beauty', 'simply', 'refuses', 'fade', 'price', 'tag', 'on', 'pretty', 'staggering', 'must', 'say', 'but', 'if', 'you', 'are', 'going', 'buy', 'any', 'cd', 'much', 'money', 'only', 'one', 'feel', 'be', 'worth', 'every', 'penny', 'amazing', 'favorite', 'all', 'time', 'hands', 'down', 'intense', 'sadness', 'prisoners', 'fate', 'which', 'means', 'more', 'have', 'played', 'hope', 'distant', 'promise', 'girl', 'who', 'stole', 'star', 'an', 'important', 'inspiration', 'me', 'personally', 'throughout']\n",
            "least frequent words:  ['survivial', 'doria', 'atocha', 'regratably', 'apollyon', 'sharkboy', 'lavagirl', 'lautner', 'paslay', 'rolemodels']\n"
          ]
        }
      ],
      "source": [
        "print('random Review: ', random_review)\n",
        "print('random Review length: ', len(random_review))\n",
        "print('-------\\n')\n",
        "print('vectorized review: ',text_vectorizer([random_review]))\n",
        "\n",
        "print('Reverse vectorized review: ',id_to_word(text_vectorizer([random_review])))\n",
        "\n",
        "\n",
        "print('-------\\n')\n",
        "print('Vocabulary_length: ',len(text_vectorizer.get_vocabulary()))\n",
        "print('Most frequent words: ',text_vectorizer.get_vocabulary()[:100])\n",
        "print('least frequent words: ',text_vectorizer.get_vocabulary()[-10:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc4d9dc6",
      "metadata": {
        "id": "fc4d9dc6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "cb2beeaa",
      "metadata": {
        "id": "cb2beeaa"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "e8afb1b4",
      "metadata": {
        "id": "e8afb1b4"
      },
      "outputs": [],
      "source": [
        "\n",
        "TEST_SIZE = .01 # same as config.TEST_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "00421d84",
      "metadata": {
        "id": "00421d84"
      },
      "outputs": [],
      "source": [
        "xtrain, xtest, ytrain, ytest = train_test_split( df.select('review'), df.select('polarity'), test_size=  .01,  random_state = SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "2f5f852a",
      "metadata": {
        "id": "2f5f852a"
      },
      "outputs": [],
      "source": [
        "del(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "46770b0b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46770b0b",
        "outputId": "dfa494f6-e717-4e57-aaca-07385926c49d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "xtrain shape (3563999, 1) ytrain shape (3563999, 1)\n",
            "xtest shape (36000, 1) ytest shape (36000, 1)\n"
          ]
        }
      ],
      "source": [
        "print('xtrain shape',xtrain.shape, 'ytrain shape', ytrain.shape)\n",
        "print('xtest shape',xtest.shape, 'ytest shape', ytest.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1fdd75f",
      "metadata": {
        "id": "e1fdd75f"
      },
      "source": [
        "# Creating tensorflow dataset using `tf.data` api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "629e79dd",
      "metadata": {
        "id": "629e79dd"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "58bd4829",
      "metadata": {
        "id": "58bd4829"
      },
      "outputs": [],
      "source": [
        "def data_generator(x, y):\n",
        "    num_samples = len(x)\n",
        "    for i in range(num_samples):\n",
        "        yield x[i], y[i]\n",
        "\n",
        "\n",
        "def create_datasets(x, y, text_vectorizer, batch_size:int = 32, shuffle:bool=True, n_repeat:int = 0, buffer_size:int=1_000_000):\n",
        "\n",
        "    generator = data_generator(x, y)\n",
        "    print('Generating...')\n",
        "    train_dataset = tf.data.Dataset.from_generator(\n",
        "        lambda: generator,\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(None, x.shape[1]), dtype=tf.string),\n",
        "            tf.TensorSpec(shape=(None, y.shape[1]), dtype=tf.int32)\n",
        "        )\n",
        "    )\n",
        "    print('Mapping...')\n",
        "    train_dataset = train_dataset.map(lambda x, y: (tf.cast(text_vectorizer(x), tf.int32)[0], y[0]), tf.data.AUTOTUNE)\n",
        "    train_dataset = train_dataset.batch(batch_size)\n",
        "\n",
        "    if shuffle:\n",
        "        train_dataset = train_dataset.shuffle(buffer_size)\n",
        "\n",
        "    if n_repeat > 0:\n",
        "        train_dataset = train_dataset.cache().repeat(n_repeat).prefetch(tf.data.AUTOTUNE)\n",
        "    elif n_repeat == -1:\n",
        "        train_dataset = train_dataset.cache().repeat().prefetch(tf.data.AUTOTUNE)\n",
        "    elif n_repeat == 0:\n",
        "        train_dataset = train_dataset.cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return train_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PxF-JUhkWFU0",
      "metadata": {
        "id": "PxF-JUhkWFU0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "92MKJzzfX84D",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92MKJzzfX84D",
        "outputId": "627346c4-b3d9-4e56-950d-07b038dd9a97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating...\n",
            "Mapping...\n"
          ]
        }
      ],
      "source": [
        "test_dataset = create_datasets(xtest, ytest, text_vectorizer, batch_size=BATCH_SIZE, n_repeat = 0, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "c8eb76f9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8eb76f9",
        "outputId": "cc0391ee-0e84-42fe-83c8-458c70f7ba08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating...\n",
            "Mapping...\n"
          ]
        }
      ],
      "source": [
        "train_dataset = create_datasets(xtrain, ytrain, text_vectorizer, batch_size=BATCH_SIZE, n_repeat=2, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "cc64ea57",
      "metadata": {
        "id": "cc64ea57"
      },
      "outputs": [],
      "source": [
        "# del(xtrain, ytrain)\n",
        "# del(xtest, ytest, counter_object)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f557ad0b",
      "metadata": {
        "id": "f557ad0b"
      },
      "outputs": [],
      "source": [
        "# train_features = tf.data.Dataset.from_tensor_slices(xtrain)\n",
        "# train_label = tf.data.Dataset.from_tensor_slices(ytrain)\n",
        "# test_features = tf.data.Dataset.from_tensor_slices(xtest)\n",
        "# test_label = tf.data.Dataset.from_tensor_slices(ytest)\n",
        "\n",
        "# del(xtrain, ytrain, xtest, ytest) # deleting variables to free the memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cfcac6b",
      "metadata": {
        "id": "8cfcac6b"
      },
      "outputs": [],
      "source": [
        "# train_dataset = tf.data.Dataset.zip((train_features, train_label))\n",
        "# train_dataset = train_dataset.map(lambda x,y: (text_vectorizer(x)[0],y),tf.data.AUTOTUNE)\n",
        "# train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# test_dataset = tf.data.Dataset.zip((test_features, test_label))\n",
        "# test_dataset = test_dataset.map(lambda x,y: (text_vectorizer(x)[0],y),tf.data.AUTOTUNE )\n",
        "# test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# del(train_features, train_label, test_features, test_label) # deleting variables to free the memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "2bb2ea74",
      "metadata": {
        "id": "2bb2ea74"
      },
      "outputs": [],
      "source": [
        "# print('len train dataset: ', len(train_dataset)) # doesn't work for generator generated tensor data\n",
        "# print('len test dataset: ', len(test_dataset))  # doesn't work for generator generated tensor data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "67cd8c53",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67cd8c53",
        "outputId": "4071e4fa-b9f1-4e5c-a7fb-e917a14220d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-01 15:44:14.149077: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_5' with dtype string\n",
            "\t [[{{node Placeholder/_5}}]]\n",
            "2023-09-01 15:44:14.151450: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_6' with dtype int64\n",
            "\t [[{{node Placeholder/_6}}]]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 175) (32, 1)\n",
            "tf.Tensor(\n",
            "[[ 9220    43  1159 ...     0     0     0]\n",
            " [   11   393   201 ...     0     0     0]\n",
            " [  120   221   413 ...     0     0     0]\n",
            " ...\n",
            " [ 1039    20    43 ...     0     0     0]\n",
            " [  411 18968     8 ...     0     0     0]\n",
            " [    1  1431    75 ...     0     0     0]], shape=(32, 175), dtype=int32) tf.Tensor(\n",
            "[[1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]], shape=(32, 1), dtype=int32)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-01 15:44:14.615734: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
          ]
        }
      ],
      "source": [
        "for i,j in train_dataset:\n",
        "    print(i.shape,j.shape)\n",
        "    print(i,j)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "f99904b6",
      "metadata": {
        "id": "f99904b6"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'xtrain' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_length \u001b[39m=\u001b[39m xtrain\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m BATCH_SIZE\n\u001b[1;32m      2\u001b[0m test_length \u001b[39m=\u001b[39m xtest\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m BATCH_SIZE\n",
            "\u001b[0;31mNameError\u001b[0m: name 'xtrain' is not defined"
          ]
        }
      ],
      "source": [
        "train_length = xtrain.shape[0]// BATCH_SIZE\n",
        "test_length = xtest.shape[0] // BATCH_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "6e76e0da",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e76e0da",
        "outputId": "12c90644-389c-4f29-95cb-17efee2e2409"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train length:  111374 \n",
            "test_length:  1125\n"
          ]
        }
      ],
      "source": [
        "print(\"train length: \", train_length, '\\ntest_length: ', test_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "8afc7914",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8afc7914",
        "outputId": "68d5556d-85f8-4958-8b51-3da6198de2b8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PosixPath('/home/t/aproject/Amazon-review-sentiment-analysis/model/text_vectorizer.pkl')"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "VECTORIZER_PATH = model_dir / 'text_vectorizer.pkl'\n",
        "\n",
        "if colab:\n",
        "  VECTORIZER_PATH = save_dir / 'text_vectorizer.pkl'\n",
        "\n",
        "VECTORIZER_PATH\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "d2f12270",
      "metadata": {
        "id": "d2f12270"
      },
      "outputs": [],
      "source": [
        "# to load text vectorizer\n",
        "def load_text_vectorizer(vectorizer_path):\n",
        "    from_disk = pickle.load(open(vectorizer_path, \"rb\"))\n",
        "    return TextVectorization.from_config(from_disk['config'])\n",
        "\n",
        "# Pickle the config and weights\n",
        "def save_text_vectorizer(vectorizer_path):\n",
        "    pickle.dump({'config': text_vectorizer.get_config()}\n",
        "                , open(vectorizer_path, \"wb\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "295c9169",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "295c9169",
        "outputId": "0c68db03-3e99-4894-a66f-7d23b6d26688"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(100000, ['', '[UNK]', 'the', 'best'])"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_vectorizer.vocabulary_size(), text_vectorizer.get_vocabulary()[:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a474f71",
      "metadata": {
        "id": "2a474f71"
      },
      "outputs": [],
      "source": [
        "# text_vectorizer = load_text_vectorizer(model_dir/'text_vectorizer.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "lAJKVqVPZzZP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAJKVqVPZzZP",
        "outputId": "a523396c-74d2-4381-e602-ad454a0d1a08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: /home/t/miniconda3/envs/deep/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "total 26M\n",
            "drwxrwxr-x  2 t t 4.0K Sep  1 14:31 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
            "drwxrwxr-x 10 t t 4.0K Aug 31 18:50 \u001b[01;34m..\u001b[0m/\n",
            "-rw-rw-r--  1 t t  12M Sep  1 14:30 counter.pkl\n",
            "-rw-rw-r--  1 t t 9.3M Sep  1 14:30 full_model.h5\n",
            "-rw-rw-r--  1 t t 3.1M Sep  1 14:30 model_weights.h5\n",
            "-rw-rw-r--  1 t t 1.1M Sep  1 14:30 text_vectorizer.pkl\n"
          ]
        }
      ],
      "source": [
        "ls -la -h ../model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "68473aa6",
      "metadata": {
        "id": "68473aa6"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "94fa1db9",
      "metadata": {
        "id": "94fa1db9"
      },
      "outputs": [],
      "source": [
        "save_text_vectorizer(VECTORIZER_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3f2c5d8",
      "metadata": {
        "id": "d3f2c5d8"
      },
      "source": [
        "# Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "54b35527",
      "metadata": {
        "id": "54b35527"
      },
      "outputs": [],
      "source": [
        "DIM = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "4cac4f8d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cac4f8d",
        "outputId": "6268a3f5-9ef8-4944-a531-389bb12469a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedded text vectorized random sentence:  tf.Tensor(\n",
            "[[ 0.04292144  0.00113931  0.01899529 ... -0.02021828  0.03969901\n",
            "   0.01261326]\n",
            " [-0.01892058 -0.01329989  0.04517606 ... -0.02116247 -0.048376\n",
            "  -0.03167375]\n",
            " [ 0.04843697  0.02070308 -0.04972619 ... -0.01345571 -0.01083864\n",
            "   0.03726724]\n",
            " ...\n",
            " [-0.02272482 -0.03220202 -0.02965466 ... -0.00393594 -0.01867509\n",
            "   0.01087759]\n",
            " [-0.02272482 -0.03220202 -0.02965466 ... -0.00393594 -0.01867509\n",
            "   0.01087759]\n",
            " [-0.02272482 -0.03220202 -0.02965466 ... -0.00393594 -0.01867509\n",
            "   0.01087759]], shape=(175, 8), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "embedding = Embedding(input_dim = MAX_TOKEN,output_dim= DIM, mask_zero=True, input_length=OUTPUT_SEQUENCE_LENGTH)\n",
        "print('Embedded text vectorized random sentence: ',embedding(text_vectorizer(random_review)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0af21ff9",
      "metadata": {
        "id": "0af21ff9"
      },
      "source": [
        "# Model:0 (Naive bayes model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "a8c97769",
      "metadata": {
        "id": "a8c97769"
      },
      "outputs": [],
      "source": [
        "# model0 = Pipeline([\n",
        "#     ('tfidf',TfidfVectorizer()),\n",
        "#     ('multino',MultinomialNB())\n",
        "# ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "2131ed31",
      "metadata": {
        "id": "2131ed31"
      },
      "outputs": [],
      "source": [
        "# %%time\n",
        "# #fit and predict\n",
        "# model0.fit(xtrain['review'], ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "0f9b520c",
      "metadata": {
        "id": "0f9b520c"
      },
      "outputs": [],
      "source": [
        "# pred0 = model0.predict(xtest['review'])\n",
        "\n",
        "# print(pred0.shape ==  ytest.shape)\n",
        "# print('pred00.shape: ',pred0.shape)\n",
        "# print('ytest.shape: ',ytest.shape)\n",
        "\n",
        "# model0_res = calculate_results(y_true=ytest, y_pred=pred0, model_name='model0: naive bayes')\n",
        "# print(model0_res)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adb0a7ec",
      "metadata": {
        "id": "adb0a7ec"
      },
      "source": [
        "{'model': 'model0: naive bayes', 'accuracy': 84.79444444444444, 'precision': 0.8482905354316962, 'recall': 0.8479444444444444, 'f1': 0.8479237632137091, 'discription': None}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "f4c5276d",
      "metadata": {
        "id": "f4c5276d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "849fff07",
      "metadata": {
        "id": "849fff07"
      },
      "source": [
        "# Model1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "ad696d7b",
      "metadata": {
        "id": "ad696d7b"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_lstm_model(input_shape, max_tokens, dim):\n",
        "    inputs = keras.Input(shape=(input_shape))\n",
        "    embedding_layer = Embedding(input_dim=max_tokens, output_dim=dim, mask_zero=True, input_length=input_shape, name='embedding_layer')(inputs)\n",
        "    x = LSTM(16, return_sequences=True, name = 'lstm_layer_1')(embedding_layer)\n",
        "    x = LSTM(16, name = 'lstm_layer_2')(x)\n",
        "    x = Dropout(0.4, name ='dropout_layer')(x)\n",
        "    x = Dense(64, activation='relu', name = 'dense_layer_1')(x)\n",
        "    outputs = Dense(1, activation='sigmoid', name = 'dense_layer_2_final')(x)\n",
        "    return keras.Model(inputs=inputs, outputs=outputs, name='model_lstm')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "c847589e",
      "metadata": {
        "id": "c847589e"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class LSTMModel(tf.keras.Model):\n",
        "    def __init__(self, text_vectorizer, input_shape, max_tokens, dim, dropout_rate=0.4):\n",
        "        super(CustomLSTMModel, self).__init__()\n",
        "        self.text_vectorizer = text_vectorizer        \n",
        "        self.embedding_layer = tf.keras.layers.Embedding(\n",
        "            input_dim=max_tokens, output_dim=dim, mask_zero=True, input_length=input_shape,\n",
        "            name='embedding_layer'\n",
        "        )\n",
        "        self.lstm_layer_1 = tf.keras.layers.LSTM(16, return_sequences=True, name='lstm_layer_1')\n",
        "        self.lstm_layer_2 = tf.keras.layers.LSTM(16, name='lstm_layer_2')\n",
        "        self.dropout_layer = tf.keras.layers.Dropout(dropout_rate, name='dropout_layer')\n",
        "        self.dense_layer_1 = tf.keras.layers.Dense(64, activation='relu', name='dense_layer_1')\n",
        "        self.dense_layer_2_final = tf.keras.layers.Dense(1, activation='sigmoid', name='dense_layer_2_final')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.embedding_layer(inputs)\n",
        "        x = self.lstm_layer_1(x)\n",
        "        x = self.lstm_layer_2(x)\n",
        "        x = self.dropout_layer(x)\n",
        "        x = self.dense_layer_1(x)\n",
        "        return self.dense_layer_2_final(x)\n",
        "\n",
        "    def predict_sentiment(self, title, text, preprocess_func=clean_text):\n",
        "        review = f'{title} {text}' # concatenate the title and text\n",
        "        clean_review = preprocess_func(review)\n",
        "        review_sequence = self.text_vectorizer([clean_review])\n",
        "        prediction = self(review_sequence)\n",
        "        sentiment_score = prediction[0][0]\n",
        "        sentiment_label = 'Positive' if sentiment_score >= 0.5 else 'Negative'\n",
        "        return sentiment_label, sentiment_score\n",
        "    \n",
        "\n",
        "# Instantiate the model\n",
        "output_sequence_length = OUTPUT_SEQUENCE_LENGTH  # actual value\n",
        "model = LSTMModel(text_vectorizer, input_dim = output_sequence_length, max_tokens=MAX_TOKEN,dim=DIM)\n",
        "# model = create_lstm_model(output_sequence_length, max_tokens=MAX_TOKEN, dim=DIM)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=['Accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "e538e2ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e538e2ec",
        "outputId": "f6140774-1189-435e-cc3a-6ffee4a439a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_lstm\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 175)]             0         \n",
            "                                                                 \n",
            " embedding_layer (Embedding)  (None, 175, 8)           800000    \n",
            "                                                                 \n",
            " lstm_layer_1 (LSTM)         (None, 175, 16)           1600      \n",
            "                                                                 \n",
            " lstm_layer_2 (LSTM)         (None, 16)                2112      \n",
            "                                                                 \n",
            " dropout_layer (Dropout)     (None, 16)                0         \n",
            "                                                                 \n",
            " dense_layer_1 (Dense)       (None, 64)                1088      \n",
            "                                                                 \n",
            " dense_layer_2_final (Dense)  (None, 1)                65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 804,865\n",
            "Trainable params: 804,865\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "d49f77c2",
      "metadata": {
        "id": "d49f77c2"
      },
      "outputs": [],
      "source": [
        "# tf.keras.utils.plot_model(model, expand_nested=True, show_shapes=True, show_dtype=True, rankdir='TP')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "rPqCfwyaF6xx",
      "metadata": {
        "id": "rPqCfwyaF6xx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "IbH83I1iGaj7",
      "metadata": {
        "id": "IbH83I1iGaj7"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "cd51a60c",
      "metadata": {
        "id": "cd51a60c"
      },
      "outputs": [],
      "source": [
        "# Define the ModelCheckpoint callback\n",
        "checkpoint_filepath = model_dir / \"model_weights.h5\"\n",
        "if colab:\n",
        "    checkpoint_filepath = save_dir / \"model_weights.h5\"\n",
        "\n",
        "model_checkpoint_callback = ModelCheckpoint(checkpoint_filepath, monitor = 'loss', save_best_only=True, save_weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "88cd1aa8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "88cd1aa8",
        "outputId": "41622832-6a47-470f-84cd-c81c93b4fe22"
      },
      "outputs": [],
      "source": [
        "# model = tf.keras.models.load_model(model_dir/'model.h5')\n",
        "model.load_weights(checkpoint_filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "6f16ab59",
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[71], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Compile the model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model1\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mBinaryCrossentropy(),\n\u001b[1;32m      5\u001b[0m               optimizer\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(),\n\u001b[1;32m      6\u001b[0m               metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mAccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m----> 8\u001b[0m model1\u001b[39m.\u001b[39;49mload_weights(checkpoint_filepath)\n",
            "File \u001b[0;32m~/miniconda3/envs/deep/lib/python3.11/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m~/miniconda3/envs/deep/lib/python3.11/site-packages/keras/saving/legacy/save.py:476\u001b[0m, in \u001b[0;36mload_weights\u001b[0;34m(model, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m    472\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`load_weights` requires h5py package when loading weights \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    473\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfrom HDF5. Try installing h5py.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    474\u001b[0m     )\n\u001b[1;32m    475\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model\u001b[39m.\u001b[39m_is_graph_network \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m model\u001b[39m.\u001b[39mbuilt:\n\u001b[0;32m--> 476\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    477\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to load weights saved in HDF5 format into a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    478\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msubclassed Model which has not created its variables yet. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    479\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCall the Model first, then load the weights.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    480\u001b[0m     )\n\u001b[1;32m    481\u001b[0m model\u001b[39m.\u001b[39m_assert_weights_created()\n\u001b[1;32m    482\u001b[0m \u001b[39mwith\u001b[39;00m h5py\u001b[39m.\u001b[39mFile(filepath, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
            "\u001b[0;31mValueError\u001b[0m: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights."
          ]
        }
      ],
      "source": [
        "model1 = LSTMModel(text_vectorizer, embed_dim=DIM)\n",
        "\n",
        "# Compile the model\n",
        "model1.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=['Accuracy'])\n",
        "\n",
        "model1.load_weights(checkpoint_filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "2nT_DHxDIBhn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nT_DHxDIBhn",
        "outputId": "862399b2-a6d2-4674-edef-f40264997108"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs          :  10\n",
            "train_length    : 111374\n",
            "test_length     :    1125\n",
            "steps_per_epoch :  1113\n",
            "validation_steps:  112\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 10\n",
        "steps_per_epoch = int(0.1 * train_length//EPOCHS)\n",
        "validation_steps = int(1.0 * test_length//EPOCHS)\n",
        "\n",
        "print('Epochs          : ', EPOCHS)\n",
        "print('train_length    :',train_length )\n",
        "print('test_length     :   ',test_length )\n",
        "print('steps_per_epoch : ',steps_per_epoch)\n",
        "print('validation_steps: ',validation_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "ba48c155",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba48c155",
        "outputId": "7f72cbff-a444-4182-c4f3-ffe74601f852",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1113/1113 [==============================] - 32s 28ms/step - loss: 0.0052 - Accuracy: 0.9986 - val_loss: 0.3441 - val_Accuracy: 0.9343\n",
            "Epoch 2/10\n",
            "1113/1113 [==============================] - 30s 27ms/step - loss: 5.6735e-05 - Accuracy: 1.0000 - val_loss: 0.4278 - val_Accuracy: 0.9309\n",
            "Epoch 3/10\n",
            "1113/1113 [==============================] - 32s 29ms/step - loss: 1.8509e-05 - Accuracy: 1.0000 - val_loss: 0.4830 - val_Accuracy: 0.9320\n",
            "Epoch 4/10\n",
            "1113/1113 [==============================] - 32s 29ms/step - loss: 1.2131e-05 - Accuracy: 1.0000 - val_loss: 0.5372 - val_Accuracy: 0.9326\n",
            "Epoch 5/10\n",
            "1113/1113 [==============================] - 35s 32ms/step - loss: 5.5844e-06 - Accuracy: 1.0000 - val_loss: 0.5923 - val_Accuracy: 0.9314\n",
            "Epoch 6/10\n",
            "1113/1113 [==============================] - 33s 29ms/step - loss: 1.1391e-05 - Accuracy: 1.0000 - val_loss: 0.6375 - val_Accuracy: 0.9303\n",
            "Epoch 7/10\n",
            "1113/1113 [==============================] - 32s 29ms/step - loss: 3.9369e-06 - Accuracy: 1.0000 - val_loss: 0.7144 - val_Accuracy: 0.9303\n",
            "Epoch 8/10\n",
            "1113/1113 [==============================] - 33s 30ms/step - loss: 3.0541e-06 - Accuracy: 1.0000 - val_loss: 0.7856 - val_Accuracy: 0.9309\n",
            "Epoch 9/10\n",
            "1113/1113 [==============================] - 34s 30ms/step - loss: 5.2775e-07 - Accuracy: 1.0000 - val_loss: 0.8326 - val_Accuracy: 0.9309\n",
            "Epoch 10/10\n",
            "1113/1113 [==============================] - 32s 28ms/step - loss: 3.6667e-06 - Accuracy: 1.0000 - val_loss: 0.8759 - val_Accuracy: 0.9309\n"
          ]
        }
      ],
      "source": [
        "# %%time\n",
        "\n",
        "# fit the model\n",
        "history1 = model.fit(train_dataset, epochs = EPOCHS,\n",
        "                      validation_data= test_dataset,\n",
        "                      steps_per_epoch = steps_per_epoch,\n",
        "                      validation_steps=validation_steps,\n",
        "                     callbacks=[model_checkpoint_callback]\n",
        "                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bb5f853",
      "metadata": {
        "id": "4bb5f853"
      },
      "outputs": [],
      "source": [
        "# model_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "efd80b56",
      "metadata": {
        "id": "efd80b56"
      },
      "outputs": [],
      "source": [
        "model.save(save_dir / 'full_model.h5')\n",
        "# model.load_weights(checkpoint_filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "ffdqjRAqvIQV",
      "metadata": {
        "id": "ffdqjRAqvIQV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "4ad33a34",
      "metadata": {
        "id": "4ad33a34"
      },
      "outputs": [],
      "source": [
        "from funcyou.plot import plot_history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a20bc95",
      "metadata": {
        "id": "4a20bc95"
      },
      "source": [
        "#### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "D0A9yrmvssfA",
      "metadata": {
        "id": "D0A9yrmvssfA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "d7621b8d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7621b8d",
        "outputId": "dce513d9-604f-4ce7-81f4-e162abeab96f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1125/1125 [==============================] - 34s 27ms/step\n",
            "ypred1.shape:  (36000,)\n"
          ]
        }
      ],
      "source": [
        "ypred = tf.squeeze(tf.round(model.predict(test_dataset)))\n",
        "print('ypred1.shape: ',ypred.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "p0WIgpWcxyD_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0WIgpWcxyD_",
        "outputId": "09d58521-121f-4029-b67b-2200e040ae70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'model': 'model1: LSTM', 'accuracy': 94.0638888888889, 'precision': 0.940657243735739, 'recall': 0.9406388888888889, 'f1': 0.9406394430686046, 'discription': 'small lstm model with vectorizer and embedding layer'}\n"
          ]
        }
      ],
      "source": [
        "model_res = calculate_results(ytest,ypred, model_name='model1: LSTM', discription = 'small lstm model with vectorizer and embedding layer')\n",
        "print(model_res)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7R6QTEVyF9R",
      "metadata": {
        "id": "f7R6QTEVyF9R"
      },
      "source": [
        "{'model': 'model1: LSTM', 'accuracy': 94.0638888888889, 'precision': 0.940657243735739, 'recall': 0.9406388888888889, 'f1': 0.9406394430686046, 'discription': 'small lstm model with vectorizer and embedding layer'}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "ab116338",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1125/1125 [==============================] - 38s 33ms/step\n",
            "ypred1.shape:  (36000,)\n"
          ]
        }
      ],
      "source": [
        "ypred1 = tf.squeeze(tf.round(model1.predict(test_dataset)))\n",
        "print('ypred1.shape: ',ypred1.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "0fd99ba0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'model': 'model1: LSTM', 'accuracy': 99.66666666666667, 'precision': 0.9966827306204571, 'recall': 0.9966666666666667, 'f1': 0.9966666976237895, 'discription': 'small lstm model with vectorizer and embedding layer'}\n"
          ]
        }
      ],
      "source": [
        "model1_res = calculate_results(ytest,ypred1, model_name='model1: LSTM', discription = 'small lstm model with vectorizer and embedding layer')\n",
        "print(model1_res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "22b4318b",
      "metadata": {
        "id": "22b4318b"
      },
      "outputs": [],
      "source": [
        "\n",
        "def predict_sentiment(title, text, text_vectorizer, lstm_model):\n",
        "    review = f'{title} {text}' # concatenate the title and text\n",
        "    clean_review = clean_text(review)\n",
        "    review_sequence = text_vectorizer([clean_review])\n",
        "    prediction = lstm_model.predict(review_sequence)\n",
        "    sentiment_score = prediction[0][0]\n",
        "    sentiment_label = 'Positive' if sentiment_score >= 0.5 else 'Negative'\n",
        "    return sentiment_label, sentiment_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "T9EhMLJPttY2",
      "metadata": {
        "id": "T9EhMLJPttY2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "NBA57rbztnUv",
      "metadata": {
        "id": "NBA57rbztnUv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "QMVdkWittyv-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QMVdkWittyv-",
        "outputId": "76957520-5dbb-4943-a234-607c0a871b66"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'i am do not y ll will not'"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = \"i'm don't , y'll, won't\"\n",
        "clean_text(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "39102a82",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39102a82",
        "outputId": "168458f6-5042-4197-e040-007083215190"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 27ms/step\n",
            "Negative 0.04752389\n"
          ]
        }
      ],
      "source": [
        "review_title = \"don't be sad\"\n",
        "review_text  =  \"man\"\n",
        "\n",
        "sentiment_label, sentiment_score = predict_sentiment(review_title, review_text, text_vectorizer, model)\n",
        "print(sentiment_label, sentiment_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29833e0e",
      "metadata": {
        "id": "29833e0e"
      },
      "source": [
        "# Final training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "0e7cb132",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e7cb132",
        "outputId": "6b1edc9a-d598-4fe6-95a3-8745898de1c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1125/1125 [==============================] - 38s 34ms/step - loss: 0.1597 - Accuracy: 0.9438\n",
            "Epoch 2/10\n",
            "1125/1125 [==============================] - 30s 26ms/step - loss: 0.1127 - Accuracy: 0.9604\n",
            "Epoch 3/10\n",
            "1125/1125 [==============================] - 30s 26ms/step - loss: 0.0852 - Accuracy: 0.9719\n",
            "Epoch 4/10\n",
            "1125/1125 [==============================] - 29s 25ms/step - loss: 0.0620 - Accuracy: 0.9813\n",
            "Epoch 5/10\n",
            "1125/1125 [==============================] - 29s 26ms/step - loss: 0.0428 - Accuracy: 0.9885\n",
            "Epoch 6/10\n",
            "1125/1125 [==============================] - 29s 26ms/step - loss: 0.0291 - Accuracy: 0.9932\n",
            "Epoch 7/10\n",
            "1125/1125 [==============================] - 31s 27ms/step - loss: 0.0203 - Accuracy: 0.9954\n",
            "Epoch 8/10\n",
            "1125/1125 [==============================] - 28s 25ms/step - loss: 0.0148 - Accuracy: 0.9969\n",
            "Epoch 9/10\n",
            "1125/1125 [==============================] - 29s 25ms/step - loss: 0.0108 - Accuracy: 0.9977\n",
            "Epoch 10/10\n",
            "1125/1125 [==============================] - 29s 26ms/step - loss: 0.0069 - Accuracy: 0.9987\n"
          ]
        }
      ],
      "source": [
        "history1 = model.fit(test_dataset, epochs = EPOCHS,\n",
        "                     callbacks=[model_checkpoint_callback]\n",
        "                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "6T4Vx9SczWYk",
      "metadata": {
        "id": "6T4Vx9SczWYk"
      },
      "outputs": [],
      "source": [
        "model.save(save_dir / 'full_model.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "id": "qqdMLoUO00E7",
      "metadata": {
        "id": "qqdMLoUO00E7"
      },
      "outputs": [],
      "source": [
        "model.save_weights(checkpoint_filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FHxxe7pN04Kj",
      "metadata": {
        "id": "FHxxe7pN04Kj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "deep",
      "language": "python",
      "name": "deep"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
